<div class="section">
    <h3 class="section-title">Notes on Calculations</h3>

    <div class="summary-card">
        <div class="summary-title">Vector Accuracy Improvement Calculation</div>
        <p>The percentage improvement is calculated by first determining the absolute reduction in distance - subtracting the new method's {{ "%.10f"|format(metrics.avg_distilled_distance|default(0)) }} distance from the legacy method's {{ "%.10f"|format(metrics.avg_chunk_distance|default(0)) }} distance to get {{ "%.8f"|format((metrics.avg_chunk_distance|default(0) - metrics.avg_distilled_distance|default(0))) }} units - and then calculating what fraction {{ "%.8f"|format((metrics.avg_chunk_distance|default(0) - metrics.avg_distilled_distance|default(0))) }} is of the original {{ "%.10f"|format(metrics.avg_chunk_distance|default(0)) }} distance. This fraction ({{ "%.8f"|format((metrics.avg_chunk_distance|default(0) - metrics.avg_distilled_distance|default(0))) }} / {{ "%.10f"|format(metrics.avg_chunk_distance|default(0)) }}) is multiplied by 100 to convert it into a percentage, resulting in a {{ "%.5f"|format(((metrics.avg_chunk_distance|default(0.45) - metrics.avg_distilled_distance|default(0.09)) / metrics.avg_chunk_distance|default(0.45) * 100)|default(80)) }}% improvement in vector distance (accuracy).</p>
    </div>

    <div class="summary-card">
        <div class="summary-title">Distilled versus Undistilled IdeaBlocks Calculation</div>
        <p>We further determined that distilling the information using Blockify's Intelligent Distillation does not negatively impact vector search accuracy and instead improves vector search accuracy when comparing a Blockify Distilled dataset versus an Undistilled dataset by +{{ "%.5f"|format(((metrics.avg_undistilled_distance|default(0.15) - metrics.avg_distilled_distance|default(0.09)) / metrics.avg_undistilled_distance|default(0.15) * 100)|default(40)) }}% Improvement in Accuracy</p>
    </div>

    <div class="summary-card">
        <div class="summary-title">Details on the Distance Calculation</div>
        <p>To calculate the distance to the best match we use a script to determine the cosine similarity between two numerical vectors. It begins by converting each input vector from a string format into an array of numbers and verifies that both vectors have the same number of dimensions.</p>
        <p>The cosine similarity is then computed by first determining the dot product of the two vectors (the sum of the products of each corresponding pair of elements) and independently calculating the Euclidean norm (or magnitude) of each vector by taking the square root of the sum of the squares of its components.</p>
        <p>Finally, the cosine similarity is obtained by dividing the dot product by the product of the two magnitudes, yielding a value that measures the orientation similarity between the vectors regardless of their scale, as represented by the formula cos(theta) = (A . B) / (||A|| ||B||).</p>
    </div>

    <div class="summary-card">
        <div class="summary-title">Details on the Best Match Calculation</div>
        <p>To provide an ultra conservative and non-subjective determination of vector accuracy improvement, the "Best Matching" Chunk DOES NOT mean it is the most textually accurate for the User Query. For impartiality, the same applies to the "Best Matching" IdeaBlock.</p>
        <p>"Best Matching" only means it is the closest in vector search distance between the User Query and the chunk in question. This approach avoids subjectivity in choosing one chunk over another for more favorable results.</p>
        <p>Note this would also mean that the LLM may receive less optimal data for its response synthesis which would also lower quality for the Legacy RAG response.</p>
        <p>Additionally because we are conducting the test with a single document in the vector dataset, the results will be heavily idealized to the benefit of the chunked text because other documents could have made the best match even less relevant than when using a single document.</p>
        <p>This process begins by analyzing the vector embeddings of the Chunks / IdeaBlocks and the vector embedding of the user's question where each row contains a question and an array of all the candidate answers along with their vector representations.</p>
        <p>For every entry, the program uses the vector numerical arrays for mathematical comparison. The script calculates each question's cosine distance between the question's vector and every candidate answer's vector, which quantitatively measures their similarity.</p>
        <p>The candidate answer with the smallest cosine distance (a sign of highest similarity) is selected as the best match, and that distance is output by the script.</p>
        <p>We can then calculate the average distance for best match across all questions in the dataset to determine overall vector distance (which determines overall accuracy).</p>
    </div>

    <h4>Aggregate Performance Calculation</h4>
    <div class="executive-summary" style="font-family: monospace; font-size: 14px;">
        <strong>Formula:</strong><br><br>
        Aggregate Performance = Vector Improvement x Word Improvement<br><br>

        <strong>This Benchmark:</strong><br>
        Aggregate Performance = {{ metrics.vector_improvement|format_decimal(2) }} x {{ metrics.word_improvement|format_decimal(2) }} = <span class="improvement-positive">{{ metrics.aggregate_performance|format_decimal(2) }}X</span>
    </div>

    <h4>Enterprise Performance Projection</h4>
    <p>
        Enterprise environments typically exhibit significant data duplication across departments,
        document versions, and systems. Industry research (IDC) indicates duplication ratios of
        8:1 to 22:1 in enterprise data stores. We use a conservative factor of {{ config.enterprise_dup_factor }}X.
    </p>

    <div class="executive-summary" style="font-family: monospace; font-size: 14px;">
        <strong>Formula:</strong><br><br>
        Enterprise Performance = Aggregate Performance x Enterprise Duplication Factor<br><br>

        <strong>This Benchmark:</strong><br>
        Enterprise Performance = {{ metrics.aggregate_performance|format_decimal(2) }} x {{ config.enterprise_dup_factor }} = <span class="improvement-positive">{{ metrics.enterprise_performance|format_decimal(2) }}X</span>
    </div>

    <h4>Token Cost Calculation</h4>
    <div class="executive-summary" style="font-family: monospace; font-size: 14px;">
        <strong>Formula:</strong><br><br>
        Annual Tokens = Tokens per Item x User Queries x Results per Query<br>
        Cost Savings = (Chunk Tokens - Block Tokens) x Price per Million / 1,000,000<br><br>

        <strong>Parameters:</strong><br>
        - Character to Token Ratio: 4:1<br>
        - Results per Query: 5<br>
        - Token Price: {{ config.token_cost_per_million|format_currency }} per million<br>
        - Annual Queries: {{ config.number_of_user_queries|format_number }}<br><br>

        <strong>This Benchmark:</strong><br>
        Cost Savings = ({{ metrics.annual_chunk_tokens|format_number }} - {{ metrics.annual_distilled_tokens|format_number }}) x {{ config.token_cost_per_million }} / 1,000,000 = <span class="improvement-positive">{{ metrics.cost_savings_per_year|format_currency }}/year</span>
    </div>

    <h4>Benchmark Conditions</h4>
    <p>
        This benchmark uses conservative methodology to ensure fair comparison:
    </p>
    <ul>
        <li><strong>Same embedding model</strong> (text-embedding-3-small) for all content types</li>
        <li><strong>Same distance metric</strong> (cosine similarity) for all comparisons</li>
        <li><strong>Queries extracted from IdeaBlocks</strong> (critical questions) to ensure relevance</li>
        <li><strong>Best-match selection</strong> (minimum distance) for each query</li>
    </ul>
</div>
